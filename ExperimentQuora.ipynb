{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model.Train import Trainer\n",
    "from model.Loss import LSmoothing\n",
    "from torch.utils.data import DataLoader\n",
    "from model.Bert import BertForQuestionPairClassification\n",
    "from model.DataManager import QuoraDataset\n",
    "from transformers import BertTokenizer\n",
    "from model.NegativeSampling import RandomSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "path =\"data/quora-question-pairs/train.csv\"\n",
    "bs = 32\n",
    "bm25_sampling = True\n",
    "\n",
    "data = QuoraDataset.load_data(path, 1000)\n",
    "data['global_docno'] = data.index.astype(str)\n",
    "train_data, val_data, test_data = QuoraDataset.split_data(data)\n",
    "if bm25_sampling : \n",
    "    index_ref_tr = QuoraDataset.index_data(train_data,type_df=\"train_5\")\n",
    "    index_ref_val = QuoraDataset.index_data(val_data,type_df=\"val_5\")\n",
    "    index_ref_test = QuoraDataset.index_data(test_data,type_df=\"test_5\")\n",
    "    train_data = BM25Sampling.sample(index_ref_tr,train_data, k=9).sort_values(by=\"question1\")\n",
    "    val_data = BM25Sampling.sample(index_ref_val,val_data, k=9).sort_values(by=\"question1\")\n",
    "    test_data = BM25Sampling.sample(index_ref_test,test_data, k=9).sort_values(by=\"question1\")\n",
    "else:\n",
    "    train_data = RandomSampling.sample(train_data, k=9).sort_values(by=\"question1\")\n",
    "    val_data = RandomSampling.sample(val_data, k=9).sort_values(by=\"question1\")\n",
    "    test_data = RandomSampling.sample(test_data, k=9).sort_values(by=\"question1\")\n",
    "\n",
    "train_dataset = QuoraDataset(train_data, tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_dataset = QuoraDataset(val_data, tokenizer, max_length=128)\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "test_dataset = QuoraDataset(test_data, tokenizer, max_length=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = BertForQuestionPairClassification()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW\n",
    "loss = LSmoothing()\n",
    "trainer = Trainer()\n",
    "history = trainer.set_model(model)\\\n",
    "    .set_loader(train_loader, val_loader, None)\\\n",
    "    .set_loss_fn(loss)\\\n",
    "    .set_optimizer(optimizer)\\\n",
    "    .fit(learning_rate, epochs, CL=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history['training']['loss']\n",
    "val_loss = history['validation']['loss']\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(val_loss, label='val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(all_scores, all_targets, K):\n",
    "    # Get the indices that would sort the array\n",
    "    top_k_indices = np.argsort(all_scores)[::-1][:K]\n",
    "    # Select the top K targets\n",
    "    top_k_targets = all_targets[top_k_indices]\n",
    "    # Calculate precision\n",
    "    precision = np.mean(top_k_targets)\n",
    "    return precision\n",
    "\n",
    "def recall_at_k(all_scores, all_targets, K, num_relevant):\n",
    "    top_k_indices = np.argsort(all_scores)[::-1][:K]\n",
    "    top_k_targets = all_targets[top_k_indices]\n",
    "    # Calculate recall\n",
    "    recall = np.sum(top_k_targets) / num_relevant\n",
    "    return recall\n",
    "\n",
    "def evaluate_ranking_model(model, data_loader, K, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            # Move all inputs to the correct device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            scores = torch.softmax(outputs, dim=1)[:, 1]  # Probability of being similar\n",
    "            scores = scores.cpu().numpy()\n",
    "            targets = labels.cpu().numpy()\n",
    "\n",
    "            # Assume all entries in a batch belong to one group/query\n",
    "            num_duplicate = np.sum(targets)  # Count of relevant documents/questions\n",
    "            if num_duplicate == 0:\n",
    "                continue  # Avoid division by zero\n",
    "            \n",
    "            precision_k = precision_at_k(scores, targets, K)\n",
    "            recall_k = recall_at_k(scores, targets, K, num_duplicate)\n",
    "\n",
    "            precisions.append(precision_k)\n",
    "            recalls.append(recall_k)\n",
    "\n",
    "    # Average over all queries\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "\n",
    "    return {\n",
    "        \"precision_at_k\": avg_precision,\n",
    "        \"recall_at_k\": avg_recall\n",
    "    }\n",
    "\n",
    "metrics = evaluate_ranking_model(model, val_loader, K=5, device=device)\n",
    "print(\"Ranking Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
